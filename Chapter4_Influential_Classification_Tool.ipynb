{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter4_Influential_Classification_Tool.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPXYoXbOy5zCvMMp53nyELM"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpaeX8IXyjD9",
        "colab_type": "text"
      },
      "source": [
        "# **Implementing ResNet From Scratch**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCqMoQ7NnH4w",
        "colab_type": "text"
      },
      "source": [
        "### Data Input Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NOLXMPvyZjY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import collections\n",
        "import functools\n",
        "import glob\n",
        "import math\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "import tensorflow.keras.regularizers as regulizers\n",
        "import matplotlib.gridspec as gridspec\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras.layers import (Input, Activation, Dense, Flatten, Conv2D,\n",
        "                                     MaxPooling2D,GlobalAveragePooling2D, \n",
        "                                     AveragePooling2D, BatchNormalization, add)\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fHZ0DgQDiZN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"2\" "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-F0I-101yuRT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "outputId": "53584aec-b79f-4676-cb85-ac245080bd52"
      },
      "source": [
        "cifar_builder = tfds.builder('cifar100')\n",
        "cifar_builder.download_and_prepare()\n",
        "\n",
        "print(f'{cifar_builder.info}')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tfds.core.DatasetInfo(\n",
            "    name='cifar100',\n",
            "    version=3.0.0,\n",
            "    description='This dataset is just like the CIFAR-10, except it has 100 classes containing 600 images each. There are 500 training images and 100 testing images per class. The 100 classes in the CIFAR-100 are grouped into 20 superclasses. Each image comes with a \"fine\" label (the class to which it belongs) and a \"coarse\" label (the superclass to which it belongs).',\n",
            "    homepage='https://www.cs.toronto.edu/~kriz/cifar.html',\n",
            "    features=FeaturesDict({\n",
            "        'coarse_label': ClassLabel(shape=(), dtype=tf.int64, num_classes=20),\n",
            "        'image': Image(shape=(32, 32, 3), dtype=tf.uint8),\n",
            "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=100),\n",
            "    }),\n",
            "    total_num_examples=60000,\n",
            "    splits={\n",
            "        'test': 10000,\n",
            "        'train': 50000,\n",
            "    },\n",
            "    supervised_keys=('image', 'label'),\n",
            "    citation=\"\"\"@TECHREPORT{Krizhevsky09learningmultiple,\n",
            "        author = {Alex Krizhevsky},\n",
            "        title = {Learning multiple layers of features from tiny images},\n",
            "        institution = {},\n",
            "        year = {2009}\n",
            "    }\"\"\",\n",
            "    redistribution_info=,\n",
            ")\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpBCw9ZXzNx8",
        "colab_type": "code",
        "outputId": "230d6d0d-d85d-44ea-f6f0-6cf7b262144d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        }
      },
      "source": [
        "print(cifar_builder.info.features['label'].names)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', 'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', 'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock', 'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur', 'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', 'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion', 'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse', 'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear', 'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine', 'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose', 'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake', 'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table', 'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout', 'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFvSWt9Tz3l9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_shape = [224, 224, 3]\n",
        "batch_size = 32\n",
        "num_epochs = 300\n",
        "\n",
        "train_cifar_dataset = cifar_builder.as_dataset(split=tfds.Split.TRAIN)\n",
        "val_cifar_dataset = cifar_builder.as_dataset(split=tfds.Split.TEST)\n",
        "\n",
        "num_classes = cifar_builder.info.features['label'].num_classes\n",
        "\n",
        "num_train_images = cifar_builder.info.splits['train'].num_examples\n",
        "num_valid_images = cifar_builder.info.splits['test'].num_examples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klw-Jepd1W7S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_cifar_dataset = train_cifar_dataset.repeat(num_epochs).shuffle(10000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dCvAbyW2Gn2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _prepare_data_fn(features, input_shape, augment=False):\n",
        "    \"\"\"\n",
        "    Resize image to expected dimensions, and opt. apply some random transformations.\n",
        "    :param features:    Data\n",
        "    :param input_shape: Shape expected by the models (images will be resized accordingly)\n",
        "    :param augment:     Flag to apply some random augmentations to the images\n",
        "    :return:            Augmented Images, Labels\n",
        "    \"\"\"\n",
        "    input_shape = tf.convert_to_tensor(input_shape)\n",
        "    \n",
        "    # Tensorflow-Dataset returns batches as feature dictionaries, expected by Estimators.\n",
        "    # To train Keras models, it is more straightforward to return the batch content as tuples:\n",
        "    image = features['image']\n",
        "    label = features['label']\n",
        "    # Convert the images to float type, also scaling their values from [0, 255] to [0., 1.]:\n",
        "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "    \n",
        "    if augment:\n",
        "        # Randomly applied horizontal flip:\n",
        "        image = tf.image.random_flip_left_right(image)\n",
        "\n",
        "        # Random B/S changes:\n",
        "        image = tf.image.random_brightness(image, max_delta=0.1)\n",
        "        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n",
        "        image = tf.clip_by_value(image, 0.0, 1.0) # keeping pixel values in check\n",
        "\n",
        "        # Random resize and random crop back to expected size:\n",
        "        \n",
        "        random_scale_factor = tf.random.uniform([1], minval=1., maxval=1.4, dtype=tf.float32)\n",
        "        scaled_height = tf.cast(tf.cast(input_shape[0], tf.float32) * random_scale_factor, \n",
        "                                tf.int32)\n",
        "        scaled_width = tf.cast(tf.cast(input_shape[1], tf.float32) * random_scale_factor, \n",
        "                               tf.int32)\n",
        "        scaled_shape = tf.squeeze(tf.stack([scaled_height, scaled_width]))\n",
        "        image = tf.image.resize(image, scaled_shape)\n",
        "        image = tf.image.random_crop(image, input_shape)\n",
        "    else:\n",
        "        image = tf.image.resize(image, input_shape[:2])\n",
        "        \n",
        "    return image, label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_EcmDLe7Ayk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prepare_data_fn_for_train = functools.partial(_prepare_data_fn, \n",
        "                                              input_shape=input_shape,\n",
        "                                              augment=True)\n",
        "\n",
        "train_cifar_dataset = train_cifar_dataset.map(prepare_data_fn_for_train)\n",
        "\n",
        "# We also ask the dataset to batch the samples:\n",
        "train_cifar_dataset = train_cifar_dataset.batch(batch_size)\n",
        "\n",
        "train_cifar_dataset = train_cifar_dataset.prefetch(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XheGO5hgCH2F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prepare_data_fn_for_val = functools.partial(_prepare_data_fn,\n",
        "                                            input_shape=input_shape)\n",
        "\n",
        "val_cifar_dataset = (val_cifar_dataset\n",
        "                    .repeat()\n",
        "                    .map(prepare_data_fn_for_val, num_parallel_calls=4)\n",
        "                    .batch(batch_size)\n",
        "                    .prefetch(1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3e8tahGgD5qm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_steps_per_epoch = math.ceil(num_train_images/batch_size)\n",
        "val_steps_per_epoch = math.ceil(num_valid_images/batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQwlQR8mnNIk",
        "colab_type": "text"
      },
      "source": [
        "### ResNet Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRp6i8gyFAee",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def _res_conv(filters, \n",
        "#               kernel_size=3, \n",
        "#               padding='same', \n",
        "#               strides=1, \n",
        "#               use_relu=True, \n",
        "#               use_bias=False, \n",
        "#               name='cbr', \n",
        "#               kernel_initializer='he_normal', \n",
        "#               kernel_regularizer=regulizers.l2(1e-4)):\n",
        "#     \"\"\"\n",
        "#     Return a layer block chaining conv, batchnrom and reLU activation.\n",
        "#     :param filters:                 Number of filters.\n",
        "#     :param kernel_size:             Kernel size.\n",
        "#     :param padding:                 Convolution padding.\n",
        "#     :param strides:                 Convolution strides.\n",
        "#     :param use_relu:                Flag to apply ReLu activation at the end.\n",
        "#     :param use_bias:                Flag to use bias or not in Conv layer.\n",
        "#     :param name:                    Name suffix for the layers.\n",
        "#     :param kernel_initializer:      Kernel initialisation method name.\n",
        "#     :param kernel_regularizer:      Kernel regularizer.\n",
        "#     :return:                        Callable layer block\n",
        "#     \"\"\"\n",
        "\n",
        "#     def layer_fn(x):\n",
        "\n",
        "#         conv = Conv2D(filters=filters, \n",
        "#                       kernel_size=kernel_size, \n",
        "#                       padding=padding,\n",
        "#                       strides=strides, \n",
        "#                       use_bias=use_bias, \n",
        "#                       kernel_initializer=kernel_initializer, \n",
        "#                       kernel_regularizer=kernel_regularizer, \n",
        "#                       name=f'{name}_c')(x)\n",
        "\n",
        "#         res = BatchNormalization(axis=-1, name=f'{name}_bn')(conv)\n",
        "\n",
        "#         if use_relu:\n",
        "#             res = Activation(\"relu\", name=f'{name}_r')(res)\n",
        "#         return res\n",
        "\n",
        "#     return layer_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MmTKJnru8yE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def _merge_with_shortcut(kernel_initializer='he_normal', \n",
        "#                          kernel_regularizer=regulizers.l2(1e-4), \n",
        "#                          name='block'):\n",
        "#     \"\"\"\n",
        "#     Return a layer block which merge an input tensor and the corresponding \n",
        "#     residual output tensor from another branch.\n",
        "#     :param kernel_initializer:      Kernel initialisation method name.\n",
        "#     :param kernel_regularizer:      Kernel regularizer.\n",
        "#     :param name:                    Name suffix for the layers.\n",
        "#     :return:                        Callable layer block\n",
        "#     \"\"\"\n",
        "\n",
        "#     def layer_fn(x, x_residual):\n",
        "#         # We check if `x_residual` was scaled down. \n",
        "#         # If so, we scale `x` accordingly with a 1x1 conv:\n",
        "#         x_shape = tf.keras.backend.int_shape(x)\n",
        "#         x_residual_shape = tf.keras.backend.int_shape(x_residual)\n",
        "#         if x_shape == x_residual_shape:\n",
        "#             shortcut = x\n",
        "#         else:\n",
        "#             strides = (\n",
        "#                 # Vertical Strides\n",
        "#                 int(round(x_shape[1] / x_residual_shape[1])),\n",
        "#                 # Horizontal strides\n",
        "#                 int(round(x_shape[2] / x_residual_shape[2]))\n",
        "#             )\n",
        "#             x_residual_channels = x_residual_shape[3]\n",
        "#             shortcut = Conv2D(filters=x_residual_channels, \n",
        "#                               kernel_size=(1, 1), \n",
        "#                               padding=\"valid\", \n",
        "#                               strides=strides,\n",
        "#                               kernel_initializer=kernel_initializer, \n",
        "#                               kernel_regularizer=kernel_regularizer,\n",
        "#                               name=name + '_shortcut_c')(x)\n",
        "\n",
        "#         merge = add([shortcut, x_residual])\n",
        "#         return merge\n",
        "\n",
        "#     return layer_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2SiEwnkwere",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def _residual_block_basic(filters, \n",
        "#                           kernel_size=3, \n",
        "#                           strides=1, \n",
        "#                           use_bias=False, \n",
        "#                           name='res_basic',\n",
        "#                           kernel_initializer='he_normal', \n",
        "#                           kernel_regularizer=regulizers.l2(1e-4)):\n",
        "#     \"\"\"\n",
        "#     Return a basic residual layer block.\n",
        "#     :param filters:                 Number of filters.\n",
        "#     :param kernel_size:             Kernel size.\n",
        "#     :param strides:                 Convolution strides\n",
        "#     :param use_bias:                Flag to use bias or not in Conv layer.\n",
        "#     :param kernel_initializer:      Kernel initialisation method name.\n",
        "#     :param kernel_regularizer:      Kernel regularizer.\n",
        "#     :return:                        Callable layer block\n",
        "#     \"\"\"\n",
        "\n",
        "#     def layer_fn(x):\n",
        "#         x_conv1 = _res_conv(filters=filters, \n",
        "#                             kernel_size=kernel_size, \n",
        "#                             padding='same', \n",
        "#                             strides=strides, \n",
        "#                             use_relu=True, use_bias=use_bias,\n",
        "#                             kernel_initializer=kernel_initializer, \n",
        "#                             kernel_regularizer=kernel_regularizer,\n",
        "#                             name=name + '_cbr_1')(x)\n",
        "#         x_residual = _res_conv(filters=filters, \n",
        "#                                kernel_size=kernel_size, \n",
        "#                                padding='same', \n",
        "#                                strides=1, \n",
        "#                                use_relu=False, \n",
        "#                                use_bias=use_bias,\n",
        "#                                kernel_initializer=kernel_initializer, \n",
        "#                                kernel_regularizer=kernel_regularizer,\n",
        "#                                name=name + '_cbr_2')(x_conv1)\n",
        "#         merge = _merge_with_shortcut(kernel_initializer, \n",
        "#                                      kernel_regularizer,\n",
        "#                                      name=name)(x, x_residual)\n",
        "#         merge = Activation('relu')(merge)\n",
        "#         return merge\n",
        "\n",
        "#     return layer_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwBMbpyKx9Km",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def _residual_block_bottleneck(filters, \n",
        "#                                kernel_size=3, \n",
        "#                                strides=1, \n",
        "#                                use_bias=False, \n",
        "#                                name='res_bottleneck',\n",
        "#                                kernel_initializer='he_normal', \n",
        "#                                kernel_regularizer=regulizers.l2(1e-4)):\n",
        "#     \"\"\"\n",
        "#     Return a residual layer block with bottleneck, \n",
        "#     recommended for deep ResNets (depth > 34).\n",
        "    \n",
        "#     :param filters:                 Number of filters.\n",
        "#     :param kernel_size:             Kernel size.\n",
        "#     :param strides:                 Convolution strides\n",
        "#     :param use_bias:                Flag to use bias or not in Conv layer.\n",
        "#     :param kernel_initializer:      Kernel initialisation method name.\n",
        "#     :param kernel_regularizer:      Kernel regularizer.\n",
        "#     :return:                        Callable layer block\n",
        "#     \"\"\"\n",
        "\n",
        "#     def layer_fn(x):\n",
        "#         x_bottleneck = _res_conv(filters=filters, \n",
        "#                                  kernel_size=1, \n",
        "#                                  padding='valid', \n",
        "#                                  strides=strides, \n",
        "#                                  use_relu=True, \n",
        "#                                  use_bias=use_bias,\n",
        "#                                  kernel_initializer=kernel_initializer, \n",
        "#                                  kernel_regularizer=kernel_regularizer,\n",
        "#                                  name=name + '_cbr1')(x)\n",
        "#         x_conv = _res_conv(filters=filters, \n",
        "#                            kernel_size=kernel_size, \n",
        "#                            padding='same', \n",
        "#                            strides=1, \n",
        "#                            use_relu=True, \n",
        "#                            use_bias=use_bias,\n",
        "#                            kernel_initializer=kernel_initializer, \n",
        "#                            kernel_regularizer=kernel_regularizer,\n",
        "#                            name=name + '_cbr2')(x_bottleneck)\n",
        "#         x_residual = _res_conv(filters=filters * 4, \n",
        "#                                kernel_size=1, \n",
        "#                                padding='valid', \n",
        "#                                strides=1, \n",
        "#                                use_relu=False, \n",
        "#                                use_bias=use_bias,\n",
        "#                                kernel_initializer=kernel_initializer, \n",
        "#                                kernel_regularizer=kernel_regularizer,\n",
        "#                                name=name + '_cbr3')(x_conv)\n",
        "#         merge = _merge_with_shortcut(kernel_initializer, \n",
        "#                                      kernel_regularizer, \n",
        "#                                      name=name)(x, x_residual)\n",
        "#         merge = Activation('relu')(merge)\n",
        "#         return merge\n",
        "\n",
        "#     return layer_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3c9wsk_zcNb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def _residual_macroblock(block_fn, filters, repetitions=3, kernel_size=3, \n",
        "#                          strides_1st_block=1, use_bias=False,\n",
        "#                          kernel_initializer='he_normal', \n",
        "#                          kernel_regularizer=regulizers.l2(1e-4),\n",
        "#                          name='res_macroblock'):\n",
        "#     \"\"\"\n",
        "#     Return a layer block, composed of a repetition of `N` residual blocks.\n",
        "#     :param block_fn:               Block layer method to be used.\n",
        "#     :param repetitions:            Number of times the block should be repeated.\n",
        "#     :param filters:                Number of filters.\n",
        "#     :param kernel_size:            Kernel size.\n",
        "#     :param strides_1st_block:      Convolution strides for the 1st block.\n",
        "#     :param use_bias:               Flag to use bias or not in Conv layer.\n",
        "#     :param kernel_initializer:     Kernel initialisation method name.\n",
        "#     :param kernel_regularizer:     Kernel regularizer.\n",
        "#     :return:                       Callable layer block\n",
        "#     \"\"\"\n",
        "\n",
        "#     def layer_fn(x):\n",
        "#         for i in range(repetitions):\n",
        "#             block_name = f\"{name}_{i}\" \n",
        "#             strides = strides_1st_block if i == 0 else 1\n",
        "#             x = block_fn(filters=filters, kernel_size=kernel_size, \n",
        "#                          strides=strides, use_bias=use_bias,\n",
        "#                          kernel_initializer=kernel_initializer, \n",
        "#                          kernel_regularizer=kernel_regularizer,\n",
        "#                          name=block_name)(x)\n",
        "#         return x\n",
        "\n",
        "#     return layer_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quOjpAShzujn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def _residual_macroblock(block_fn, filters, repetitions=3, kernel_size=3, \n",
        "#                          strides_1st_block=1, use_bias=False,\n",
        "#                          kernel_initializer='he_normal', \n",
        "#                          kernel_regularizer=regulizers.l2(1e-4),\n",
        "#                          name='res_macroblock'):\n",
        "#     \"\"\"\n",
        "#     Return a layer block, composed of a repetition of `N` residual blocks.\n",
        "#     :param block_fn:               Block layer method to be used.\n",
        "#     :param repetitions:            Number of times the block should be repeated.\n",
        "#     :param filters:                Number of filters.\n",
        "#     :param kernel_size:            Kernel size.\n",
        "#     :param strides_1st_block:      Convolution strides for the 1st block.\n",
        "#     :param use_bias:               Flag to use bias or not in Conv layer.\n",
        "#     :param kernel_initializer:     Kernel initialisation method name.\n",
        "#     :param kernel_regularizer:     Kernel regularizer.\n",
        "#     :return:                       Callable layer block\n",
        "#     \"\"\"\n",
        "\n",
        "#     def layer_fn(x):\n",
        "#         for i in range(repetitions):\n",
        "#             block_name = f\"{name}_{i}\" \n",
        "#             strides = strides_1st_block if i == 0 else 1\n",
        "#             x = block_fn(filters=filters, kernel_size=kernel_size, \n",
        "#                          strides=strides, use_bias=use_bias,\n",
        "#                          kernel_initializer=kernel_initializer, \n",
        "#                          kernel_regularizer=kernel_regularizer,\n",
        "#                          name=block_name)(x)\n",
        "#         return x\n",
        "\n",
        "#     return layer_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCwHRuZh1JYl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def ResNet(input_shape, num_classes=1000, block_fn=_residual_block_basic, \n",
        "#            repetitions=(2, 2, 2, 2), use_bias=False, \n",
        "#            kernel_initializer='he_normal', \n",
        "#            kernel_regularizer=regulizers.l2(1e-4)):\n",
        "#     \"\"\"\n",
        "#     Build a ResNet model for classification.\n",
        "#     :param input_shape:             Input shape (e.g. (224, 224, 3))\n",
        "#     :param num_classes:             Number of classes to predict\n",
        "#     :param block_fn:                Block layer method to be used.\n",
        "#     :param repetitions:             List of repetitions for each macro-blocks \n",
        "#                                     the network should contain.\n",
        "#     :param use_bias:                Flag to use bias or not in Conv layer.\n",
        "#     :param kernel_initializer:      Kernel initialisation method name.\n",
        "#     :param kernel_regularizer:      Kernel regularizer.\n",
        "#     :return:                        ResNet model.\n",
        "#     \"\"\"\n",
        "\n",
        "#     # Input and 1st layers:\n",
        "#     inputs = Input(shape=input_shape)\n",
        "#     conv = _res_conv(filters=64, kernel_size=7, strides=2, use_relu=True, \n",
        "#                      use_bias=use_bias,kernel_initializer=kernel_initializer, \n",
        "#                      kernel_regularizer=kernel_regularizer)(inputs)\n",
        "#     maxpool = MaxPooling2D(pool_size=3, strides=2, padding='same')(conv)\n",
        "\n",
        "#     # Chain of residual blocks:\n",
        "#     filters = 64\n",
        "#     strides = 2\n",
        "#     res_block = maxpool\n",
        "#     for i, repet in enumerate(repetitions):\n",
        "#         # We do not further reduce the input size for the 1st block\n",
        "#         # (max-pool applied just before):\n",
        "#         block_strides = strides if i != 0 else 1\n",
        "#         macroblock_name = \"block_{}\".format(i) \n",
        "#         res_block = _residual_macroblock(\n",
        "#             block_fn=block_fn, repetitions=repet, \n",
        "#             name=macroblock_name,filters=filters, \n",
        "#             strides_1st_block=block_strides, \n",
        "#             use_bias=use_bias, kernel_initializer=kernel_initializer, \n",
        "#             kernel_regularizer=kernel_regularizer)(res_block)\n",
        "#         filters = min(filters * 2, 1024) # we limit to 1024 filters max\n",
        "\n",
        "#     # Final layers for prediction:\n",
        "#     res_spatial_dim = tf.keras.backend.int_shape(res_block)[1:3]\n",
        "#     avg_pool = AveragePooling2D(pool_size=res_spatial_dim, strides=1)(res_block)\n",
        "#     flatten = Flatten()(avg_pool)\n",
        "#     predictions = Dense(units=num_classes, \n",
        "                        \n",
        "#                         kernel_initializer=kernel_initializer, \n",
        "#                         activation='softmax')(flatten)\n",
        "\n",
        "#     # Model:\n",
        "#     model = Model(inputs=inputs, outputs=predictions)\n",
        "#     return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bR1njchh2Mia",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def ResNet18(input_shape, num_classes=1000, use_bias=True,\n",
        "#              kernel_initializer='he_normal', kernel_regularizer=None):\n",
        "#     return ResNet(input_shape, num_classes, block_fn=_residual_block_basic, \n",
        "#                   repetitions=(2, 2, 2, 2), use_bias=use_bias, \n",
        "#                   kernel_initializer=kernel_initializer, \n",
        "#                   kernel_regularizer=kernel_regularizer)\n",
        "\n",
        "\n",
        "# def ResNet34(input_shape, num_classes=1000, use_bias=True,\n",
        "#              kernel_initializer='he_normal', kernel_regularizer=None):\n",
        "#     return ResNet(input_shape, num_classes, block_fn=_residual_block_basic, \n",
        "#                   repetitions=(3, 4, 6, 3), use_bias=use_bias, \n",
        "#                   kernel_initializer=kernel_initializer, \n",
        "#                   kernel_regularizer=kernel_regularizer)\n",
        "\n",
        "\n",
        "# def ResNet50(input_shape, num_classes=1000, use_bias=True,\n",
        "#              kernel_initializer='he_normal', kernel_regularizer=None):\n",
        "#     # Note: ResNet50 is similar to ResNet34,\n",
        "#     # with the basic blocks replaced by bottleneck ones.\n",
        "#     return ResNet(input_shape, num_classes, block_fn=_residual_block_bottleneck,\n",
        "#                   repetitions=(3, 4, 6, 3), use_bias=use_bias, \n",
        "#                   kernel_initializer=kernel_initializer, \n",
        "#                   kernel_regularizer=kernel_regularizer)\n",
        "\n",
        "\n",
        "# def ResNet101(input_shape, num_classes=1000, use_bias=True,\n",
        "#              kernel_initializer='he_normal', kernel_regularizer=None):\n",
        "#     return ResNet(input_shape, num_classes, block_fn=_residual_block_bottleneck,\n",
        "#                   repetitions=(3, 4, 23, 3), use_bias=use_bias, \n",
        "#                   kernel_initializer=kernel_initializer, \n",
        "#                   kernel_regularizer=kernel_regularizer)\n",
        "\n",
        "\n",
        "# def ResNet152(input_shape, num_classes=1000, use_bias=True,\n",
        "#              kernel_initializer='he_normal', kernel_regularizer=None):\n",
        "#     return ResNet(input_shape, num_classes, block_fn=_residual_block_bottleneck,\n",
        "#                   repetitions=(3, 8, 36, 3), use_bias=use_bias, \n",
        "#                   kernel_initializer=kernel_initializer, \n",
        "#                   kernel_regularizer=kernel_regularizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydUXIL-n4i3B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# resnet50 = ResNet50(input_shape=input_shape, num_classes=num_classes)\n",
        "# resnet50.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AJON-s94nWm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# optimizer = tf.keras.optimizers.Adam()\n",
        "# accuracy_metric = tf.metrics.SparseCategoricalAccuracy(name='acc')\n",
        "# top5_accuracy_metric = tf.metrics.SparseTopKCategoricalAccuracy(\n",
        "#     k=5, name='top5_acc'\n",
        "# )\n",
        "# resnet50.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', \n",
        "#                  metrics=[accuracy_metric, top5_accuracy_metric])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGkc2dvb5dQN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setting some variables to format the logs:\n",
        "log_begin_red, log_begin_blue, log_begin_green = '\\033[91m','\\033[94m','\\033[92m'\n",
        "log_begin_bold, log_begin_underline = '\\033[1m', '\\033[4m'\n",
        "log_end_format = '\\033[0m'\n",
        "\n",
        "class SimpleLogCallback(tf.keras.callbacks.Callback):\n",
        "    \"\"\" Keras callback for simple, denser console logs.\"\"\"\n",
        "\n",
        "    def __init__(self, metrics_dict, num_epochs='?', log_frequency=1,\n",
        "                 metric_string_template=\n",
        "                 '\\033[1m[[name]]\\033[0m = \\033[94m{[[value]]:5.3f}\\033[0m'):\n",
        "        \"\"\"\n",
        "        Initialize the Callback.\n",
        "        :param metrics_dict:            Dictionary containing mappings for \n",
        "                                        metrics names/keys e.g. {\"accuracy\": \n",
        "                                        \"acc\", \"val. accuracy\": \"val_acc\"}\n",
        "        :param num_epochs:              Number of training epochs\n",
        "        :param log_frequency:           Log frequency (in epochs)\n",
        "        :param metric_string_template:  (opt.) String template to print each \n",
        "                                        metric\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.metrics_dict = collections.OrderedDict(metrics_dict)\n",
        "        self.num_epochs = num_epochs\n",
        "        self.log_frequency = log_frequency\n",
        "\n",
        "        # We build a format string to later print the metrics, \n",
        "        # (e.g. \"Epoch 0/9: loss = 1.00; val-loss = 2.00\")\n",
        "        log_string_template = 'Epoch {0:2}/{1}: '\n",
        "        separator = '; '\n",
        "\n",
        "        i = 2\n",
        "        for metric_name in self.metrics_dict:\n",
        "            templ = metric_string_template.replace(\n",
        "                '[[name]]', metric_name).replace(\n",
        "                    '[[value]]', str(i))\n",
        "            log_string_template += templ + separator\n",
        "            i += 1\n",
        "\n",
        "        # We remove the \"; \" after the last element:\n",
        "        log_string_template = log_string_template[:-len(separator)]\n",
        "        self.log_string_template = log_string_template\n",
        "\n",
        "    def on_train_begin(self, logs=None):\n",
        "        print(f\"Training: {log_begin_red}start{log_end_format}\")\n",
        "\n",
        "    def on_train_end(self, logs=None):\n",
        "        print(f\"Training: {log_begin_green}end{log_end_format}\")\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if (epoch - 1) % self.log_frequency == 0 or epoch == self.num_epochs:\n",
        "            values = [logs[self.metrics_dict[metric_name]] \n",
        "                      for metric_name in self.metrics_dict]\n",
        "            print(self.log_string_template.format(epoch, self.num_epochs, \n",
        "                                                  *values))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9Y6TtJN64QS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# metrics_to_print = collections.OrderedDict([(\"loss\", \"loss\"), \n",
        "#                                             (\"v-loss\", \"val_loss\"),\n",
        "#                                             (\"acc\", \"acc\"), \n",
        "#                                             (\"v-acc\", \"val_acc\"),\n",
        "#                                             (\"top5-acc\", \"top5_acc\"), \n",
        "#                                             (\"v-top5-acc\", \"val_top5_acc\")])\n",
        "\n",
        "# callback_simple_log = SimpleLogCallback(metrics_to_print, \n",
        "#                                         num_epochs=num_epochs, log_frequency=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00PmNqWt7zKO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model_dir = './models/resnet_from_scratch'\n",
        "# callbacks = [\n",
        "#     # Callback to interrupt the training if the validation loss/metrics \n",
        "#     # stops improving for some epochs:\n",
        "#     tf.keras.callbacks.EarlyStopping(patience=8, monitor='val_acc',\n",
        "#                                      restore_best_weights=True),\n",
        "#     # Callback to log the graph, losses and metrics into TensorBoard:\n",
        "#     tf.keras.callbacks.TensorBoard(log_dir=model_dir, histogram_freq=0, \n",
        "#                                    write_graph=True),\n",
        "#     # Callback to save the model (e.g., every 5 epochs), specifying the epoch \n",
        "#     # and val-loss in the filename:\n",
        "#     tf.keras.callbacks.ModelCheckpoint(os.path.join(model_dir, \n",
        "#                      'weights-epoch{epoch:02d}-loss{val_loss:.2f}.h5'), \n",
        "#                      save_freq=5),\n",
        "#     # Log callback:\n",
        "#     callback_simple_log \n",
        "# ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-57a59fo8JcM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# history = resnet50.fit(train_cifar_dataset,  \n",
        "#                        epochs=num_epochs, steps_per_epoch=train_steps_per_epoch,\n",
        "#                        validation_data=(val_cifar_dataset), \n",
        "#                        validation_steps=val_steps_per_epoch,\n",
        "#                        verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97aU_rWf8rut",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fig, ax = plt.subplots(3, 2, figsize=(15, 10), sharex='col')\n",
        "# ax[0, 0].set_title(\"loss\")\n",
        "# ax[0, 1].set_title(\"val-loss\")\n",
        "# ax[1, 0].set_title(\"acc\")\n",
        "# ax[1, 1].set_title(\"val-acc\")\n",
        "# ax[2, 0].set_title(\"top5-acc\")\n",
        "# ax[2, 1].set_title(\"val-top5-acc\")\n",
        "\n",
        "# ax[0, 0].plot(history.history['loss'])\n",
        "# ax[0, 1].plot(history.history['val_loss'])\n",
        "# ax[1, 0].plot(history.history['acc'])\n",
        "# ax[1, 1].plot(history.history['val_acc'])\n",
        "# ax[2, 0].plot(history.history['top5_acc'])\n",
        "# ax[2, 1].plot(history.history['val_top5_acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkPn1KCd8sy1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# best_val_acc = max(history.history['val_acc']) * 100\n",
        "# best_val_top5 = max(history.history['val_top5_acc']) * 100\n",
        "\n",
        "# print(f'Best val acc:  {best_val_acc:2.2f}%')\n",
        "# print(f'Best val top5: {best_val_top5:2.2f}%')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpZpgsVl9uDB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def load_image(image_path, size):\n",
        "#     \"\"\"\n",
        "#     Load an image as a Numpy array.\n",
        "#     :param image_path:  Path of the image\n",
        "#     :param size:        Target size\n",
        "#     :return             Image array, normalized between 0 and 1\n",
        "#     \"\"\"\n",
        "#     image = img_to_array(load_img(image_path, target_size=size)) / 255.\n",
        "#     return image\n",
        "\n",
        "\n",
        "# def process_predictions(class_probabilities, class_readable_labels, k=5):\n",
        "#     \"\"\"\n",
        "#     Process a batch of predictions from our estimator.\n",
        "#     :param class_probabilities:     Prediction results returned by the Keras \n",
        "#                                     classifier for a batch of data\n",
        "#     :param class_readable_labels:   List of readable-class labels, for display\n",
        "#     :param k:                       Number of top predictions to consider\n",
        "#     :return                         Readable labels and probabilities for the \n",
        "#                                     predicted classes\n",
        "#     \"\"\"\n",
        "#     topk_labels, topk_probabilities = [], []\n",
        "#     for i in range(len(class_probabilities)):\n",
        "#         # Getting the top-k predictions:\n",
        "#         topk_classes = sorted(np.argpartition(class_probabilities[i], -k)[-k:])\n",
        "    \n",
        "#         # Getting the corresponding labels and probabilities:\n",
        "#         topk_labels.append([class_readable_labels[predicted] \n",
        "#                             for predicted in topk_classes])\n",
        "#         topk_probabilities.append(class_probabilities[i][topk_classes])\n",
        "    \n",
        "#     return topk_labels, topk_probabilities\n",
        "\n",
        "\n",
        "# def display_predictions(images, topk_labels, topk_probabilities):\n",
        "#     \"\"\"\n",
        "#     Plot a batch of predictions.\n",
        "#     :param images:                  Batch of input images\n",
        "#     :param topk_labels:             String labels of predicted classes\n",
        "#     :param topk_probabilities:      Probabilities for each class\n",
        "#     \"\"\"\n",
        "#     num_images = len(images)\n",
        "#     num_images_sqrt = np.sqrt(num_images)\n",
        "#     plot_cols = plot_rows = int(np.ceil(num_images_sqrt))\n",
        "    \n",
        "#     figure = plt.figure(figsize=(13,10))\n",
        "#     grid_spec = gridspec.GridSpec(plot_cols, plot_rows)\n",
        "    \n",
        "#     for i in range(num_images):\n",
        "#         img, pred_labels, pred_proba = images[i], topk_labels[i], topk_probabilities[i]\n",
        "#         # Shortening the labels to better fit in the plot:\n",
        "#         pred_labels = [label.split(',')[0][:20] for label in pred_labels]\n",
        "        \n",
        "#         grid_spec_i = gridspec.GridSpecFromSubplotSpec(3, 1, \n",
        "#                                                        subplot_spec=grid_spec[i], \n",
        "#                                                        hspace=0.1)\n",
        "        \n",
        "#         # Drawing the input image:\n",
        "#         ax_img = figure.add_subplot(grid_spec_i[:2])\n",
        "#         ax_img.axis('off')\n",
        "#         ax_img.imshow(img)\n",
        "#         ax_img.autoscale(tight=True)\n",
        "        \n",
        "#         # Plotting a bar chart for the predictions:\n",
        "#         ax_pred = figure.add_subplot(grid_spec_i[2])\n",
        "#         ax_pred.spines['top'].set_visible(False)\n",
        "#         ax_pred.spines['right'].set_visible(False)\n",
        "#         ax_pred.spines['bottom'].set_visible(False)\n",
        "#         ax_pred.spines['left'].set_visible(False)\n",
        "#         y_pos = np.arange(len(pred_labels))\n",
        "#         ax_pred.barh(y_pos, pred_proba, align='center')\n",
        "#         ax_pred.set_yticks(y_pos)\n",
        "#         ax_pred.set_yticklabels(pred_labels)\n",
        "#         ax_pred.invert_yaxis()\n",
        "        \n",
        "#     plt.tight_layout()\n",
        "#     plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2UxbVn-_uKz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test_filenames = glob.glob(os.path.join('res', '*'))\n",
        "# test_images = np.asarray([load_image(file, size=input_shape[:2]) \n",
        "#                          for file in test_filenames])\n",
        "# print(f'Test Images: {test_images.shape}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9dmAXpo_-hM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# image_batch = test_images[:16]\n",
        "\n",
        "# # Our model was trained on CIFAR images, which originally are 32x32px. \n",
        "# # We scaled them up to 224x224px to train our model on, but this means \n",
        "# # the resulting images had important artifacts/low quality.\n",
        "# # To test on images of the same quality, we first resize them to 32x32px,\n",
        "# # then to the expected input size (i.e., 224x224px):\n",
        "# cifar_original_image_size = cifar_builder.info.features['image'].shape[:2]\n",
        "# image_batch_low_quality = tf.image.resize(image_batch, cifar_original_image_size)\n",
        "# image_batch_low_quality = tf.image.resize(image_batch_low_quality, input_shape[:2])\n",
        "    \n",
        "\n",
        "# predictions = resnet50.predict_on_batch(image_batch_low_quality)\n",
        "# print(f'Predicted class probabilities: {predictions.shape}')\n",
        "\n",
        "# class_readable_labels = cifar_builder.info.features[\"label\"].names\n",
        "# top5_labels, top5_probabilities = process_predictions(predictions, \n",
        "#                                                       class_readable_labels)\n",
        "    \n",
        "# display_predictions(image_batch, top5_labels, top5_probabilities)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uSf3vgXlEHz",
        "colab_type": "text"
      },
      "source": [
        "# Reusing Models from Keras Applications"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JJHa1jslSwL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_shape = [224, 224, 3]\n",
        "batch_size = 32\n",
        "num_epochs = 300\n",
        "random_seed = 42"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z027QM7Ll0X7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4058a3c7-bfe2-4931-f308-4029ffe2a5b4"
      },
      "source": [
        "resnet50 = tf.keras.applications.resnet.ResNet50(\n",
        "    include_top=True, \n",
        "    weights=None, \n",
        "    input_shape=input_shape, \n",
        "    classes=num_classes\n",
        ")\n",
        "resnet50.summary()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"resnet50\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1_pad (ZeroPadding2D)       (None, 230, 230, 3)  0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1_conv (Conv2D)             (None, 112, 112, 64) 9472        conv1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1_bn (BatchNormalization)   (None, 112, 112, 64) 256         conv1_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1_relu (Activation)         (None, 112, 112, 64) 0           conv1_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pad (ZeroPadding2D)       (None, 114, 114, 64) 0           conv1_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pool (MaxPooling2D)       (None, 56, 56, 64)   0           pool1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_conv (Conv2D)    (None, 56, 56, 64)   4160        pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_relu (Activation (None, 56, 56, 64)   0           conv2_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_relu (Activation (None, 56, 56, 64)   0           conv2_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_conv (Conv2D)    (None, 56, 56, 256)  16640       pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_add (Add)          (None, 56, 56, 256)  0           conv2_block1_0_bn[0][0]          \n",
            "                                                                 conv2_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_out (Activation)   (None, 56, 56, 256)  0           conv2_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_relu (Activation (None, 56, 56, 64)   0           conv2_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_relu (Activation (None, 56, 56, 64)   0           conv2_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_add (Add)          (None, 56, 56, 256)  0           conv2_block1_out[0][0]           \n",
            "                                                                 conv2_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_out (Activation)   (None, 56, 56, 256)  0           conv2_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_relu (Activation (None, 56, 56, 64)   0           conv2_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_relu (Activation (None, 56, 56, 64)   0           conv2_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_add (Add)          (None, 56, 56, 256)  0           conv2_block2_out[0][0]           \n",
            "                                                                 conv2_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_out (Activation)   (None, 56, 56, 256)  0           conv2_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_conv (Conv2D)    (None, 28, 28, 128)  32896       conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_relu (Activation (None, 28, 28, 128)  0           conv3_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_relu (Activation (None, 28, 28, 128)  0           conv3_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_conv (Conv2D)    (None, 28, 28, 512)  131584      conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_add (Add)          (None, 28, 28, 512)  0           conv3_block1_0_bn[0][0]          \n",
            "                                                                 conv3_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_out (Activation)   (None, 28, 28, 512)  0           conv3_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_relu (Activation (None, 28, 28, 128)  0           conv3_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_relu (Activation (None, 28, 28, 128)  0           conv3_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_add (Add)          (None, 28, 28, 512)  0           conv3_block1_out[0][0]           \n",
            "                                                                 conv3_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_out (Activation)   (None, 28, 28, 512)  0           conv3_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_relu (Activation (None, 28, 28, 128)  0           conv3_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_relu (Activation (None, 28, 28, 128)  0           conv3_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_add (Add)          (None, 28, 28, 512)  0           conv3_block2_out[0][0]           \n",
            "                                                                 conv3_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_out (Activation)   (None, 28, 28, 512)  0           conv3_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_relu (Activation (None, 28, 28, 128)  0           conv3_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_relu (Activation (None, 28, 28, 128)  0           conv3_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_add (Add)          (None, 28, 28, 512)  0           conv3_block3_out[0][0]           \n",
            "                                                                 conv3_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_out (Activation)   (None, 28, 28, 512)  0           conv3_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_conv (Conv2D)    (None, 14, 14, 256)  131328      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_relu (Activation (None, 14, 14, 256)  0           conv4_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_relu (Activation (None, 14, 14, 256)  0           conv4_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_conv (Conv2D)    (None, 14, 14, 1024) 525312      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_0_bn[0][0]          \n",
            "                                                                 conv4_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_out (Activation)   (None, 14, 14, 1024) 0           conv4_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_relu (Activation (None, 14, 14, 256)  0           conv4_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_relu (Activation (None, 14, 14, 256)  0           conv4_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_out[0][0]           \n",
            "                                                                 conv4_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_out (Activation)   (None, 14, 14, 1024) 0           conv4_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_relu (Activation (None, 14, 14, 256)  0           conv4_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_relu (Activation (None, 14, 14, 256)  0           conv4_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_add (Add)          (None, 14, 14, 1024) 0           conv4_block2_out[0][0]           \n",
            "                                                                 conv4_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_out (Activation)   (None, 14, 14, 1024) 0           conv4_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_relu (Activation (None, 14, 14, 256)  0           conv4_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_relu (Activation (None, 14, 14, 256)  0           conv4_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_add (Add)          (None, 14, 14, 1024) 0           conv4_block3_out[0][0]           \n",
            "                                                                 conv4_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_out (Activation)   (None, 14, 14, 1024) 0           conv4_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_relu (Activation (None, 14, 14, 256)  0           conv4_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_relu (Activation (None, 14, 14, 256)  0           conv4_block5_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block5_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_add (Add)          (None, 14, 14, 1024) 0           conv4_block4_out[0][0]           \n",
            "                                                                 conv4_block5_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_out (Activation)   (None, 14, 14, 1024) 0           conv4_block5_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block5_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_relu (Activation (None, 14, 14, 256)  0           conv4_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_relu (Activation (None, 14, 14, 256)  0           conv4_block6_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block6_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_add (Add)          (None, 14, 14, 1024) 0           conv4_block5_out[0][0]           \n",
            "                                                                 conv4_block6_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_out (Activation)   (None, 14, 14, 1024) 0           conv4_block6_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_conv (Conv2D)    (None, 7, 7, 512)    524800      conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_relu (Activation (None, 7, 7, 512)    0           conv5_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_relu (Activation (None, 7, 7, 512)    0           conv5_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_conv (Conv2D)    (None, 7, 7, 2048)   2099200     conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_0_bn[0][0]          \n",
            "                                                                 conv5_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_out (Activation)   (None, 7, 7, 2048)   0           conv5_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_relu (Activation (None, 7, 7, 512)    0           conv5_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_relu (Activation (None, 7, 7, 512)    0           conv5_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_out[0][0]           \n",
            "                                                                 conv5_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_out (Activation)   (None, 7, 7, 2048)   0           conv5_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_relu (Activation (None, 7, 7, 512)    0           conv5_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_relu (Activation (None, 7, 7, 512)    0           conv5_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_add (Add)          (None, 7, 7, 2048)   0           conv5_block2_out[0][0]           \n",
            "                                                                 conv5_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_out (Activation)   (None, 7, 7, 2048)   0           conv5_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "avg_pool (GlobalAveragePooling2 (None, 2048)         0           conv5_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "predictions (Dense)             (None, 100)          204900      avg_pool[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 23,792,612\n",
            "Trainable params: 23,739,492\n",
            "Non-trainable params: 53,120\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJkw94Jopb4g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining a custom top5 accuracy\n",
        "accuracy_metric = tf.metrics.SparseCategoricalAccuracy(\n",
        "    name='acc'\n",
        ")\n",
        "top5_accuracy_metric = tf.metrics.SparseTopKCategoricalAccuracy(\n",
        "    k=5, \n",
        "    name='top5_acc'\n",
        ")\n",
        "\n",
        "# Metrics to print to stdout\n",
        "metrics_to_print = collections.OrderedDict(\n",
        "    [\n",
        "     ('loss', 'loss'),\n",
        "     ('v-loss', 'val_loss'),\n",
        "     ('acc', 'acc'),\n",
        "     ('v-acc', 'val_acc'),\n",
        "     ('top5-acc', 'top5_acc'),\n",
        "     ('v-top5-acc', 'val_top5_acc')\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Optimizer to use during training\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "Callbacks\n",
        "model_dir = './models/resnet_keras_app'\n",
        "callbacks = [\n",
        "            #  Callback to stop training if accuracy doesn't improve\n",
        "             tf.keras.callbacks.EarlyStopping(\n",
        "                 monitor='val_acc',\n",
        "                 patience=8,\n",
        "                 restore_best_weights=True\n",
        "             ),\n",
        "            #  Callback to save model every 5 epochs\n",
        "             tf.keras.callbacks.ModelCheckpoint(\n",
        "                 model_dir,\n",
        "                 save_freq=5\n",
        "             )\n",
        "]\n",
        "\n",
        "# Compiling & training\n",
        "resnet50.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=[accuracy_metric, top5_accuracy_metric]\n",
        ")\n",
        "\n",
        "history = resnet50.fit(\n",
        "    train_cifar_dataset, \n",
        "    epochs=num_epochs, \n",
        "    steps_per_epoch=train_steps_per_epoch,\n",
        "    validation_data=val_cifar_dataset, \n",
        "    validation_steps=val_steps_per_epoch,\n",
        "    verbose=0, \n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lG4tmMvvhJc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, ax = plt.subplots(3, 2, figsize=(15,10), sharex='col')\n",
        "ax[0, 0].set_title(\"loss\")\n",
        "ax[0, 1].set_title(\"val-loss\")\n",
        "ax[1, 0].set_title(\"acc\")\n",
        "ax[1, 1].set_title(\"val-acc\")\n",
        "ax[2, 0].set_title(\"top5-acc\")\n",
        "ax[2, 1].set_title(\"val-top5-acc\")\n",
        "\n",
        "ax[0, 0].plot(history.history['loss'])\n",
        "ax[0, 1].plot(history.history['val_loss'])\n",
        "ax[1, 0].plot(history.history['acc'])\n",
        "ax[1, 1].plot(history.history['val_acc'])\n",
        "ax[2, 0].plot(history.history['top5_acc'])\n",
        "ax[2, 1].plot(history.history['val_top5_acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dH775lo-0n5T",
        "colab_type": "text"
      },
      "source": [
        "# Fetching Models From TensorFlow Hub"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJsLV8D90mpo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "module_url = \"https://tfhub.dev/google/tf2-preview/inception_v3/feature_vector/2\"\n",
        "inception_expected_input_shape = [299, 299, 3]\n",
        "inception_expected_output_shape = [2048]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yFHU0da1WF8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "fe76eb3e-30e7-4262-c68c-28f0c919caee"
      },
      "source": [
        "hub_feature_extractor = hub.KerasLayer(\n",
        "    module_url, \n",
        "    trainable=False,                              # Flag to set the layers as trainable or not\n",
        "    input_shape=inception_expected_input_shape,   # Expected input shape.\n",
        "    output_shape=inception_expected_output_shape, # Output shape [batch_size, 2048].\n",
        "    dtype=tf.float32)                             # Expected dtype\n",
        "\n",
        "# Note: These parameters can be found on the webpage of tfhub Module, or can be fetched as follows:\n",
        "# module_spec = hub.load_module_spec(model_url)\n",
        "# expected_height, expected_width = hub.get_expected_image_size(module_spec)\n",
        "# expected_input_shape = tf.convert_to_tensor([height, width, 3])\n",
        "\n",
        "print(hub_feature_extractor)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<tensorflow_hub.keras_layer.KerasLayer object at 0x7fdb2e3f95f8>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0Id2qMq1uKU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inception_model = Sequential([\n",
        "    hub_feature_extractor,\n",
        "    Dense(num_classes, activation='softmax', name=\"logits_pred\")\n",
        "], name=\"inception_tf_hub\") "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgVTTjUu2lvX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "outputId": "f63c8d55-e8e6-47d2-ba86-c575fdb7a5a4"
      },
      "source": [
        "inception_model.summary()"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"inception_tf_hub\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "keras_layer (KerasLayer)     (None, 2048)              21802784  \n",
            "_________________________________________________________________\n",
            "logits_pred (Dense)          (None, 100)               204900    \n",
            "=================================================================\n",
            "Total params: 22,007,684\n",
            "Trainable params: 204,900\n",
            "Non-trainable params: 21,802,784\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcsY8Sis2nyi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compile:\n",
        "inception_model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=[\n",
        "        tf.keras.metrics.SparseCategoricalAccuracy(name='acc'),\n",
        "        tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5, name='top5_acc')\n",
        "    ])\n",
        "\n",
        "# Train\n",
        "history = inception_model.fit(\n",
        "    train_cifar_dataset,  epochs=num_epochs, steps_per_epoch=train_steps_per_epoch,\n",
        "    validation_data=val_cifar_dataset, validation_steps=val_steps_per_epoch,\n",
        "    verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uiPOB0MDAFB",
        "colab_type": "text"
      },
      "source": [
        "# Applying Transfer Learning\n",
        "\n",
        "Transfer Learning is commonly used to pretrain a ConvNet on a very large dataset & then use the ConvNet either as an Initialization or a fixed feature extractor for the task of interest. Three scenarios where Transfer Learning can be used are:\n",
        "\n",
        "- The ConvNet as a fixed feature extractor can be pretrained on a large dataset by removing the last FC layer then treat the rest of the layers as a feature extractor for the new dataset.\n",
        "\n",
        "- Fine-tuning the ConvNet is the second strategy to not only replace and retrain the classifier on top of the ConvNet on the new dataset, but to also fine-tune the weights of the pretrained network by continuing the backpropagation.\n",
        "\n",
        "- Dataset is small & Transfer Learning can prevent Overfitting.\n",
        "\n",
        "- New dataset is very similar to the dataset on which the model was trained.\n",
        "\n",
        "- New dataset is large but different from the dataset on which the model was pretrained on.\n",
        "\n",
        "_**Resources**_:\n",
        "\n",
        "- [Trasnfer Learning - CS231N](https://cs231n.github.io/transfer-learning/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZaYFsoz3Jcr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "from matplotlib import pyplot as plt\n",
        "import math\n",
        "\n",
        "# Some hyper-parameters:\n",
        "batch_size  = 32            # Images per batch (reduce/increase according to the machine's capability)\n",
        "num_epochs  = 300           # Max number of training epochs\n",
        "random_seed = 42            # Seed for some random operations, for reproducibility"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EaUQq5KFRPGV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "287619e9-01bb-428d-881e-f5b584b372a7"
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
        "\n",
        "resnet50_feature_extractor = tf.keras.applications.resnet50.ResNet50(\n",
        "    include_top=False, weights='imagenet', input_shape=input_shape)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94773248/94765736 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xv-liJ7dRhj5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "frozen_layers, trainable_layers = [], []\n",
        "for layer in resnet50_feature_extractor.layers:\n",
        "    if isinstance(layer, tf.keras.layers.Conv2D):\n",
        "        layer.trainable = False\n",
        "        frozen_layers.append(layer.name)\n",
        "    else:\n",
        "        if len(layer.trainable_weights) > 0:\n",
        "            # We list as \"trainable\" only the layers with trainable parameters.\n",
        "            trainable_layers.append(layer.name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxDy9rEWRl61",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "outputId": "81880267-8e5b-4641-ca61-032971609c81"
      },
      "source": [
        "log_begin_red, log_begin_blue, log_begin_green = '\\033[91m', '\\n\\033[94m', '\\033[92m'\n",
        "log_begin_bold, log_begin_underline = '\\033[1m', '\\033[4m'\n",
        "log_end_format = '\\033[0m'\n",
        "\n",
        "# Logging the lists of frozen/trainable layers:\n",
        "print(\"{2}Layers we froze:{4} {0} ({3}total = {1}{4}).\".format(\n",
        "    frozen_layers, len(frozen_layers), log_begin_red, log_begin_bold, log_end_format))\n",
        "print(\"\\n{2}Layers which will be fine-tuned:{4} {0} ({3}total = {1}{4}).\".format(\n",
        "    trainable_layers, len(trainable_layers), log_begin_blue, log_begin_bold, log_end_format))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[91mLayers we froze:\u001b[0m ['conv1_conv', 'conv2_block1_1_conv', 'conv2_block1_2_conv', 'conv2_block1_0_conv', 'conv2_block1_3_conv', 'conv2_block2_1_conv', 'conv2_block2_2_conv', 'conv2_block2_3_conv', 'conv2_block3_1_conv', 'conv2_block3_2_conv', 'conv2_block3_3_conv', 'conv3_block1_1_conv', 'conv3_block1_2_conv', 'conv3_block1_0_conv', 'conv3_block1_3_conv', 'conv3_block2_1_conv', 'conv3_block2_2_conv', 'conv3_block2_3_conv', 'conv3_block3_1_conv', 'conv3_block3_2_conv', 'conv3_block3_3_conv', 'conv3_block4_1_conv', 'conv3_block4_2_conv', 'conv3_block4_3_conv', 'conv4_block1_1_conv', 'conv4_block1_2_conv', 'conv4_block1_0_conv', 'conv4_block1_3_conv', 'conv4_block2_1_conv', 'conv4_block2_2_conv', 'conv4_block2_3_conv', 'conv4_block3_1_conv', 'conv4_block3_2_conv', 'conv4_block3_3_conv', 'conv4_block4_1_conv', 'conv4_block4_2_conv', 'conv4_block4_3_conv', 'conv4_block5_1_conv', 'conv4_block5_2_conv', 'conv4_block5_3_conv', 'conv4_block6_1_conv', 'conv4_block6_2_conv', 'conv4_block6_3_conv', 'conv5_block1_1_conv', 'conv5_block1_2_conv', 'conv5_block1_0_conv', 'conv5_block1_3_conv', 'conv5_block2_1_conv', 'conv5_block2_2_conv', 'conv5_block2_3_conv', 'conv5_block3_1_conv', 'conv5_block3_2_conv', 'conv5_block3_3_conv'] (\u001b[1mtotal = 53\u001b[0m).\n",
            "\n",
            "\n",
            "\u001b[94mLayers which will be fine-tuned:\u001b[0m ['conv1_bn', 'conv2_block1_1_bn', 'conv2_block1_2_bn', 'conv2_block1_0_bn', 'conv2_block1_3_bn', 'conv2_block2_1_bn', 'conv2_block2_2_bn', 'conv2_block2_3_bn', 'conv2_block3_1_bn', 'conv2_block3_2_bn', 'conv2_block3_3_bn', 'conv3_block1_1_bn', 'conv3_block1_2_bn', 'conv3_block1_0_bn', 'conv3_block1_3_bn', 'conv3_block2_1_bn', 'conv3_block2_2_bn', 'conv3_block2_3_bn', 'conv3_block3_1_bn', 'conv3_block3_2_bn', 'conv3_block3_3_bn', 'conv3_block4_1_bn', 'conv3_block4_2_bn', 'conv3_block4_3_bn', 'conv4_block1_1_bn', 'conv4_block1_2_bn', 'conv4_block1_0_bn', 'conv4_block1_3_bn', 'conv4_block2_1_bn', 'conv4_block2_2_bn', 'conv4_block2_3_bn', 'conv4_block3_1_bn', 'conv4_block3_2_bn', 'conv4_block3_3_bn', 'conv4_block4_1_bn', 'conv4_block4_2_bn', 'conv4_block4_3_bn', 'conv4_block5_1_bn', 'conv4_block5_2_bn', 'conv4_block5_3_bn', 'conv4_block6_1_bn', 'conv4_block6_2_bn', 'conv4_block6_3_bn', 'conv5_block1_1_bn', 'conv5_block1_2_bn', 'conv5_block1_0_bn', 'conv5_block1_3_bn', 'conv5_block2_1_bn', 'conv5_block2_2_bn', 'conv5_block2_3_bn', 'conv5_block3_1_bn', 'conv5_block3_2_bn', 'conv5_block3_3_bn'] (\u001b[1mtotal = 53\u001b[0m).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6a8n4dR7Rrf0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features = resnet50_feature_extractor.output\n",
        "avg_pool = GlobalAveragePooling2D(data_format='channels_last')(features)\n",
        "predictions = Dense(num_classes, activation='softmax')(avg_pool)\n",
        "\n",
        "resnet50_freeze = Model(resnet50_feature_extractor.input, predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOeUWH5HRuHc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import collections\n",
        "import functools\n",
        "\n",
        "metrics_to_print = collections.OrderedDict([(\"loss\", \"loss\"), \n",
        "                                            (\"v-loss\", \"val_loss\"),\n",
        "                                            (\"acc\", \"acc\"), \n",
        "                                            (\"v-acc\", \"val_acc\"),\n",
        "                                            (\"top5-acc\", \"top5_acc\"), \n",
        "                                            (\"v-top5-acc\", \"val_top5_acc\")])\n",
        "\n",
        "# Compile:\n",
        "optimizer = tf.keras.optimizers.SGD(momentum=0.9, nesterov=True)\n",
        "resnet50_freeze.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=[\n",
        "        tf.keras.metrics.SparseCategoricalAccuracy(name='acc'),\n",
        "        tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5, name='top5_acc')\n",
        "    ])\n",
        "\n",
        "# Train:\n",
        "history_freeze = resnet50_freeze.fit(\n",
        "    train_cifar_dataset,  epochs=num_epochs, steps_per_epoch=train_steps_per_epoch,\n",
        "    validation_data=val_cifar_dataset, validation_steps=val_steps_per_epoch,\n",
        "    verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-D_wQOTR6jH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, ax = plt.subplots(3, 2, figsize=(15,10), sharex='col')\n",
        "ax[0, 0].set_title(\"loss\")\n",
        "ax[0, 1].set_title(\"val-loss\")\n",
        "ax[1, 0].set_title(\"acc\")\n",
        "ax[1, 1].set_title(\"val-acc\")\n",
        "ax[2, 0].set_title(\"top5-acc\")\n",
        "ax[2, 1].set_title(\"val-top5-acc\")\n",
        "\n",
        "ax[0, 0].plot(history_freeze.history['loss'])\n",
        "ax[0, 1].plot(history_freeze.history['val_loss'])\n",
        "ax[1, 0].plot(history_freeze.history['acc'])\n",
        "ax[1, 1].plot(history_freeze.history['val_acc'])\n",
        "ax[2, 0].plot(history_freeze.history['top5_acc'])\n",
        "ax[2, 1].plot(history_freeze.history['val_top5_acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTW0VyDQR_H_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for layer in resnet50_feature_extractor.layers:\n",
        "    if 'res5' in layer.name:\n",
        "        # Keras developers named the layers in their ResNet implementation to explicitly \n",
        "        # identify which macro-block and block each layer belongs to.\n",
        "        # If we reach a layer which has a name starting by 'resnet5', it means we reached \n",
        "        # the 4th macro-block / we are done with the 3rd one (see layer names listed previously):\n",
        "        break\n",
        "    if isinstance(layer, tf.keras.layers.Conv2D):\n",
        "        layer.trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJyzwPvASCd6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_macroblocks_to_freeze = [0, 1, 2, 3] # we already covered the \"all 4 frozen\" case above.\n",
        "\n",
        "histories = dict()\n",
        "histories['freeze all'] = history_freeze\n",
        "for freeze_num in num_macroblocks_to_freeze:\n",
        "        \n",
        "    print(\"{1}{2}>> {3}ResNet-50 with {0} macro-block(s) frozen{4}:\".format(\n",
        "        freeze_num, log_begin_green, log_begin_bold, log_begin_underline, log_end_format))\n",
        "    \n",
        "    # ---------------------\n",
        "    # 1. We instantiate a new classifier each time:\n",
        "    resnet50_feature_extractor = tf.keras.applications.resnet50.ResNet50(\n",
        "        include_top=False, weights='imagenet', \n",
        "        input_shape=input_shape, classes=num_classes)\n",
        "\n",
        "    features = resnet50_feature_extractor.output\n",
        "    avg_pool = GlobalAveragePooling2D(data_format='channels_last')(features)\n",
        "    predictions = Dense(num_classes, activation='softmax')(avg_pool)\n",
        "\n",
        "    resnet50_finetune = Model(resnet50_feature_extractor.input, predictions)\n",
        "    \n",
        "    # ---------------------\n",
        "    # 2. We freeze the desired layers: \n",
        "    break_layer_name = 'res{}'.format(freeze_num + 2) if freeze_num > 0 else 'conv1'\n",
        "    frozen_layers = []\n",
        "    for layer in resnet50_finetune.layers:\n",
        "        if break_layer_name in layer.name:\n",
        "            break\n",
        "        if isinstance(layer, tf.keras.layers.Conv2D):\n",
        "            # If the layer is a convolution, and isn't after the 1st layer not to train:\n",
        "            layer.trainable = False\n",
        "            frozen_layers.append(layer.name)\n",
        "    \n",
        "    print(\"\\t> {2}Layers we froze:{4} {0} ({3}total = {1}{4}).\".format(\n",
        "        frozen_layers, len(frozen_layers), log_begin_red, log_begin_bold, log_end_format))\n",
        "    \n",
        "    # ---------------------\n",
        "    # 3. To start from the beginning the data iteration, \n",
        "    #    we re-instantiate the input pipelines (same parameters):\n",
        "    train_cifar_dataset = cifar_utils.get_dataset(\n",
        "    phase='train', batch_size=batch_size, num_epochs=num_epochs, shuffle=True,\n",
        "    input_shape=input_shape, seed=random_seed)\n",
        "\n",
        "    val_cifar_dataset = cifar_utils.get_dataset(\n",
        "        phase='test', batch_size=batch_size, num_epochs=1, shuffle=False,\n",
        "        input_shape=input_shape, seed=random_seed)\n",
        "\n",
        "    # ---------------------\n",
        "    # 4. We set up the training operations, and start the process:\n",
        "    # We set a smaller learning rate for the fine-tuning:\n",
        "    # optimizer = tf.keras.optimizers.SGD(lr=1e-4, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "    optimizer = tf.keras.optimizers.SGD(momentum=0.9, nesterov=True)\n",
        "\n",
        "    model_dir = './models/resnet_keras_app_freeze_{}_mb'.format(freeze_num)\n",
        "    callbacks = [\n",
        "        # Callback to interrupt the training if the validation loss/metrics converged:\n",
        "        # (we use a shorter patience here, just to shorten a bit the demonstration, already quite long...)\n",
        "        tf.keras.callbacks.EarlyStopping(patience=8, monitor='val_acc', restore_best_weights=True),\n",
        "        # Callback to log the graph, losses and metrics into TensorBoard:\n",
        "        tf.keras.callbacks.TensorBoard(log_dir=model_dir, histogram_freq=0, write_graph=True),\n",
        "        # Callback to save the model (e.g., every 5 epochs)::\n",
        "        tf.keras.callbacks.ModelCheckpoint(\n",
        "            os.path.join(model_dir, 'weights-epoch{epoch:02d}-loss{val_loss:.2f}.h5'), period=5)\n",
        "    ]\n",
        "    \n",
        "    # Compile:\n",
        "    resnet50_finetune.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=[\n",
        "            tf.keras.metrics.SparseCategoricalAccuracy(name='acc'),\n",
        "            tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5, name='top5_acc')\n",
        "        ])\n",
        "\n",
        "    # Train:\n",
        "    print(\"\\t> Training - {0}start{1} (logs = off)\".format(log_begin_red, log_end_format))\n",
        "    history = resnet50_finetune.fit(\n",
        "        train_cifar_dataset,  epochs=num_epochs, steps_per_epoch=train_steps_per_epoch,\n",
        "        validation_data=val_cifar_dataset, validation_steps=val_steps_per_epoch,\n",
        "        verbose=0, callbacks=callbacks)\n",
        "    print(\"\\t> Training - {0}over{1}\".format(log_begin_green, log_end_format))\n",
        "\n",
        "    acc = history.history['acc'][-1] * 100\n",
        "    top5 = history.history['top5_acc'][-1] * 100\n",
        "    val_acc = history.history['val_acc'][-1] * 100\n",
        "    val_top5 = history.history['val_top5_acc'][-1] * 100\n",
        "    epochs = len(history.history['val_loss'])\n",
        "    print(\"\\t> Results after {5}{0}{6} epochs:\\t{5}acc = {1:.2f}%; top5 = {2:.2f}%; val_acc = {3:.2f}%; val_top5 = {4:.2f}%{6}\".format(\n",
        "        epochs, acc, top5, val_acc, val_top5, log_begin_bold, log_end_format))\n",
        "\n",
        "    histories['freeze {}'.format(freeze_num)] = history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52GuI9G0SGSt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, ax = plt.subplots(3, 2, figsize=(15, 10), sharex='col') # add parameter `sharey='row'` for a more direct comparison\n",
        "ax[0, 0].set_title(\"loss\")\n",
        "ax[0, 1].set_title(\"val-loss\")\n",
        "ax[1, 0].set_title(\"acc\")\n",
        "ax[1, 1].set_title(\"val-acc\")\n",
        "ax[2, 0].set_title(\"top5-acc\")\n",
        "ax[2, 1].set_title(\"val-top5-acc\")\n",
        "\n",
        "lines, labels = [], []\n",
        "for config_name in histories:\n",
        "    history = histories[config_name]\n",
        "    ax[0, 0].plot(history.history['loss'])\n",
        "    ax[0, 1].plot(history.history['val_loss'])\n",
        "    ax[1, 0].plot(history.history['acc'])\n",
        "    ax[1, 1].plot(history.history['val_acc'])\n",
        "    ax[2, 0].plot(history.history['top5_acc'])\n",
        "    line = ax[2, 1].plot(history.history['val_top5_acc'])\n",
        "    lines.append(line[0])\n",
        "    labels.append(config_name)\n",
        "\n",
        "fig.legend(lines, labels, loc='center right', borderaxespad=0.1)\n",
        "plt.subplots_adjust(right=0.87)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xd-JhikmSK-i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import glob\n",
        "import numpy as np\n",
        "from classification_utils import load_image, process_predictions, display_predictions\n",
        "\n",
        "test_filenames = glob.glob(os.path.join('res', '*'))\n",
        "test_images = np.asarray([load_image(file, size=input_shape[:2]) \n",
        "                          for file in test_filenames])\n",
        "\n",
        "image_batch = test_images[:16]\n",
        "\n",
        "# Our model was trained on CIFAR images, which originally are 32x32px. We scaled them up\n",
        "# to 224x224px to train our model on, but this means the resulting images had important\n",
        "# artifacts/low quality.\n",
        "# To test on images of the same quality, we first resize them to 32x32px, then to the \n",
        "#expected input size (i.e., 224x224px):\n",
        "cifar_original_image_size = cifar_info.features['image'].shape[:2]\n",
        "class_readable_labels = cifar_info.features[\"label\"].names\n",
        "\n",
        "image_batch_low_quality = tf.image.resize(image_batch, cifar_original_image_size)\n",
        "image_batch_low_quality = tf.image.resize(image_batch_low_quality, input_shape[:2])\n",
        "    \n",
        "predictions = resnet50_finetune.predict_on_batch(image_batch_low_quality)\n",
        "top5_labels, top5_probabilities = process_predictions(predictions, class_readable_labels)\n",
        "\n",
        "print(\"ResNet-50 trained on ImageNet and fine-tuned on CIFAR-100:\")\n",
        "display_predictions(image_batch, top5_labels, top5_probabilities)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}